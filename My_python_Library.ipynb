{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ti3kjhgtRBM"
      },
      "source": [
        "\n",
        "# **My *Python* _Library_**\n",
        "***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enZFU8YPHZj_"
      },
      "source": [
        "# **Básicos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7NGIkaZi64B"
      },
      "outputs": [],
      "source": [
        "pip install --user -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIdD5CigRTCT"
      },
      "source": [
        "## **Liberías comunes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WChfIn2b1Td"
      },
      "outputs": [],
      "source": [
        "#Imprimir con salto de línea\n",
        "print (\"1 \\n2\")\n",
        "#Instalar en terminal\n",
        "pip install pandas\n",
        "#You can import the new package and add an alias for it :\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "import sys\n",
        "!{sys.executable} -m pip install scipy\n",
        "from scipy import stats\n",
        "import math\n",
        "import base64\n",
        "import datetime\n",
        "import base64\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from shapely.geometry import Point, shape\n",
        "\n",
        "import c1applet.histdens as histdens\n",
        "hdapp = histdens.app\n",
        "\n",
        "import folium  #needed for interactive map\n",
        "from folium.plugins import HeatMap\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "# Required for basic python plotting functionality\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Advanced plotting functionality with seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "# seaborn is an extremely useful data visualization library specifically designed for statistical figures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fN76hqsvymS"
      },
      "source": [
        "## **Información básica**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4jAC4E9u73u"
      },
      "outputs": [],
      "source": [
        "#Leer un archivo csv \n",
        "df = pd.read_csv(\"data/df.csv\", sep=\",\", nrows=1000)\n",
        "df1 = pd.read_csv(\"data/df1.csv\")\n",
        "df2 = pd.read_csv(\"data/df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JorIw1oO2iqU"
      },
      "outputs": [],
      "source": [
        "#Leer un archivo json\n",
        "datajson = pd.read_json(\"data/data.json\", orient=\"columns\")\n",
        "datajson.head()\n",
        "\n",
        "datajson = []\n",
        "with open('datajson.json', encoding='utf-8')  as f:\n",
        "    for row in f.readlines()[:1000]:\n",
        "        checkins.append(json.loads(row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7NyBihj231i"
      },
      "outputs": [],
      "source": [
        "#jston to dataframe\n",
        "split_columns = datajson[\"categoricalvariable\"].str.split(pat=\",\", expand=True)\n",
        "split_columns.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXk6Y52amM3a"
      },
      "source": [
        "### **Exportar a csv en formato utf-8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pfek5lklm3l"
      },
      "outputs": [],
      "source": [
        "#Exportar la info a csv\n",
        "df.to_csv(\"data.csv\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw61-74LmFrn"
      },
      "source": [
        "UTF-8 (8-bit Unicode Transformation Format) es un formato de codificación de caracteres Unicode e ISO 10646 que utiliza símbolos de longitud variable. UTF-8 fue creado por Robert C. Pike y Kenneth L. Thompson. Está definido como estándar por la <RFC 3629> de la Internet Engineering Task Force (IETF).1​ Actualmente es una de las tres posibilidades de codificación reconocidas por Unicode y lenguajes web, o cuatro en ISO 10646.\n",
        "\n",
        "Sus características principales son:\n",
        "\n",
        "Es capaz de representar cualquier carácter Unicode.\n",
        "Usa símbolos de longitud variable (de 1 a 4 bytes por carácter Unicode).\n",
        "Incluye la especificación US-ASCII de 7 bits, por lo que cualquier mensaje ASCII se representa sin cambios.\n",
        "Incluye sincronía. Es posible determinar el inicio de cada símbolo sin reiniciar la lectura desde el principio de la comunicación.\n",
        "No superposición. Los conjuntos de valores que puede tomar cada byte de un carácter multibyte, son disjuntos, por lo que no es posible confundirlos entre sí."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbbNosgmP2T"
      },
      "source": [
        "### **Fijar una columna como index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx9MiJVrcb4l"
      },
      "outputs": [],
      "source": [
        "#It is usually a good idea to set one of the columns as the index, instead of using the default:\n",
        "\n",
        "dfindex= df.set_index('indexvariable') # In this context, Date is the best candidate to be the index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtV0Ui9_mWf7"
      },
      "source": [
        "## **Visualización inicial del archivo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "kb9iLccusM_U",
        "outputId": "86a4a4bf-d4a4-4fab-8bc4-e86e7c9ecb01"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-756d2648b5c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Take a look at the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Take a look at the columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEpktbTkvGsO"
      },
      "outputs": [],
      "source": [
        "#Ver la parte inicial del archivo \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLZXjaG-vTiS"
      },
      "outputs": [],
      "source": [
        "#Ver la parte final del archivo \n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyEPrBdcPdWZ"
      },
      "outputs": [],
      "source": [
        "#Saber el tamaño del dataframe\n",
        "df.shape\n",
        "#contar solo las filas\n",
        "count_row = df.shape[0]  # gives number of row count \n",
        "#contar solo las columnas\n",
        "count_col = df.shape[1]  # gives number of col count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHu9Vxag0-JV"
      },
      "source": [
        "###**Ver si hay datos perdidos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyHMA8NG1DxM"
      },
      "outputs": [],
      "source": [
        "df.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l1IuAmfmSDR"
      },
      "outputs": [],
      "source": [
        "#Cuántos son?\n",
        "df[\"Value\"].isna().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDfZu4MIFeLL"
      },
      "source": [
        "###**Ordenar datos por una columna**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xkgc7o6FnVZ"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by=\"variable\", ascending=False)\n",
        "# To Z-A sort, put ascending=False inside the parenthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyQsNCzIeMb"
      },
      "source": [
        "Para sacar solo los 5 primeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGcy9HSCId6X"
      },
      "outputs": [],
      "source": [
        "df[(df[\"justforthiscategoricalvariable\"] == category)].sort_values(by=\"continuosvariable\", ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvveNCAapC0F"
      },
      "source": [
        "##**Tipo de variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlz5Ri5PvdRq"
      },
      "outputs": [],
      "source": [
        "#Ver los tipos de variables que contiene el archivo\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmC9o1ttlseg"
      },
      "outputs": [],
      "source": [
        "#To inspect the data types of a DataFrame\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6fbEwzJqDdO"
      },
      "outputs": [],
      "source": [
        "variablefloat = float(variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcUmuVT6cdPs"
      },
      "outputs": [],
      "source": [
        "# Let's see the codes assigned to the categories\n",
        "df['varible'].cat.codes.unique()\n",
        "#array([ 0,  1, -1], dtype=int8)\n",
        "\"\"\" \n",
        "You can see there are 2 codes here: 0 and 1, which correspond to the two values ['A'] and ['B']. (The number -1 represents the NaN values.) In order to interpolate the values, we need to call the .interpolate() method after having replaced the -1s with actual null values (np.nans):\n",
        "\"\"\"\n",
        "# The below code replaces the value -1 with NaN\n",
        "user_cat_codes = df['userCategory'].cat.codes.replace(-1, np.nan)\n",
        "\n",
        "# We now call the interpolate function that actually fills the NaN values with either a 0 or 1\n",
        "user_cat_codes = user_cat_codes.interpolate()\n",
        "user_cat_codes\n",
        "#Now we convert the codes to the category data type:\n",
        "user_cat_codes = user_cat_codes.astype(int).astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsFhOl_gnH7f"
      },
      "source": [
        "###**Convertir el tipo de dato en una variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKEsbES-nIiJ"
      },
      "outputs": [],
      "source": [
        "#convert Value to the float64 data type\n",
        "df[\"Variable\"] = df[\"Variable\"].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpmWMAUeNf2H"
      },
      "source": [
        "###**Visualizar el data frame con opciones especiales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIKNiNYYOczP"
      },
      "outputs": [],
      "source": [
        "#Cuando me muestre una tabla, hágalo dejándome ver mínimo 100 columnas.\n",
        "pd.options.display.max_columns = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCPS8pUIHe1t"
      },
      "source": [
        "#**Preparación de las bases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HrsaDH_ynPA"
      },
      "source": [
        "##**Eliminar datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TfM9J2Umg3O"
      },
      "source": [
        "###**Eliminar una categoría**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "raDz665Vypp4",
        "outputId": "50dd3735-769b-462d-c1d3-578f653ca232"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2977c83ab4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#remove a variable column:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Varaible\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "#remove a variable column:\n",
        "df = df.drop(columns=[\"Varaible\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR-OHoTdsSWT"
      },
      "outputs": [],
      "source": [
        "#O \n",
        "dropme = df[\"Varaible\"]==\"Category\" # We filter the rows with the selected Category \n",
        "df = df.drop(df[dropme].index)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YdwmSpomjdB"
      },
      "source": [
        "###**Eliminar duplicados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDwGromoz-wb"
      },
      "outputs": [],
      "source": [
        "#remove duplicates\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPJzRXK__TwP"
      },
      "outputs": [],
      "source": [
        "#remove duplicates by a variable\n",
        "df[\"variable\"].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaCbAhQTmnMR"
      },
      "source": [
        "###**Eliminar datos perdidos nan**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0KjOLMYmsSa"
      },
      "outputs": [],
      "source": [
        "#remove all rows in which at least one entry is missing:\n",
        "df = df.dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGDesVn2unLP"
      },
      "source": [
        "##**Limpiar datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zN3TB2Y4WQQ"
      },
      "source": [
        "###**Cambiar nombre de columnas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBxZPtLX4aCp"
      },
      "outputs": [],
      "source": [
        "#Rename the columns \n",
        "columns_dict = {\"Rental Id\":\"rental_id\",\n",
        "                \"Duration\":\"duration\",\n",
        "                \"Bike Id\":\"bike_id\",\n",
        "                \"End Date\":\"end_date\",\n",
        "                \"EndStation Id\":\"end_station_id\",\n",
        "                \"Start Date\":\"start_date\",\n",
        "                \"StartStation Id\":\"start_station_id\",\n",
        "                \"userCategory\":\"user_category\"\n",
        "               }\n",
        "df = df.rename(columns=columns_dict)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eesQZh_zT1M"
      },
      "source": [
        "### **Llenar vacíos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfjjgNbs2ok_"
      },
      "source": [
        "####**Fill with nan or 0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX88jGzYt6Qa"
      },
      "outputs": [],
      "source": [
        "#Replace the missing values in variable with the \"Not Available\" string.\n",
        "df[\"variable\"] = df[\"variable\"].fillna('Not available')\n",
        "df['variable'].fillna(np.nan, inplace=True)\n",
        "\n",
        "df = df.fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkQY-xqh2xmk"
      },
      "source": [
        "####**Fill numeric values with a regression analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wqc2FXmu-ha"
      },
      "source": [
        "The interpolate() method uses linear interpolation, which is a mathematical method for filling in unknown points based on building a linear regression model on the non-missing points, as it preserves aspects of the distribution of the data, which can be very important for certain analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw_Trj5au3aP"
      },
      "outputs": [],
      "source": [
        "#First Convert to a category type\n",
        "df['userCategory'] = df['userCategory'].astype('category')\n",
        "# see what the data looks like\n",
        "df['userCategory'].cat.categories\n",
        "# Let's see the codes assigned to the categories\n",
        "df['userCategory'].cat.codes.unique()\n",
        "#array([ 0,  1, -1], dtype=int8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul0hgcntwHtg"
      },
      "source": [
        "```python\n",
        "Index(['['A']', '['B']'], dtype='object')\n",
        "array([ 0,  1, -1], dtype=int8)\n",
        "```\n",
        "\n",
        "You can see there are 2 codes here: 0 and 1, which correspond to the two values ['A'] and ['B']. (The number -1 represents the NaN values.) In order to interpolate the values, we need to call the .interpolate() method after having replaced the -1s with actual null values (np.nans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KSu5NShwvMZ"
      },
      "outputs": [],
      "source": [
        "# The below code replaces the value -1 with NaN\n",
        "user_cat_codes = df['userCategory'].cat.codes.replace(-1, np.nan)\n",
        "\n",
        "# We now call the interpolate function that actually fills the NaN values with either a 0 or 1\n",
        "user_cat_codes = user_cat_codes.interpolate()\n",
        "user_cat_codes\n",
        "\n",
        "#Now we convert the codes to the category data type\n",
        "user_cat_codes = user_cat_codes.astype(int).astype('category')\n",
        "\n",
        "#Finally, we replace back the 0s and 1s with the actual category names. \n",
        "#For this, we can use the **`.cat.rename_categories()`** method:\n",
        "user_cat_codes = user_cat_codes.cat.rename_categories(df['userCategory'].cat.categories)\n",
        "df['userCategory'] = user_cat_codes\n",
        "df['userCategory']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcPd2AGr2_wY"
      },
      "source": [
        "####**Fill categorycal variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaFghxl53HGN"
      },
      "source": [
        "We will do so by using the [IterativeImputer](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation) class of `scikit-learn` to fill the missing values in these columns. `IterativeImputer` fills one column after another by building models for each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r53bsccL3G5N"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer()\n",
        "\n",
        "cols_to_impute = ['variable1', 'variable2']\n",
        "\n",
        "imputed_df = pd.DataFrame(imputer.fit_transform(reviews[cols_to_impute]))\n",
        "imputed_df\n",
        "imputed_df.columns = cols_to_impute\n",
        "\n",
        "reviews[['stars']] = imputed_df['stars'] # could be reviews[cols_to_impute] = imputed_df[cols_to_impute]\n",
        "reviews.isnull().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPUj9XhBGXyQ"
      },
      "source": [
        "####**Una categoría fusionada, pasar a filas independientes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCkIwt0sGj3I"
      },
      "source": [
        " La ubicación era el INDEX\n",
        " \n",
        " De\n",
        "\n",
        "ubicación | cat | data1 \n",
        "--- | --- | --- \n",
        "1 | blue | 63.48  \n",
        "  | green | 62.93  \n",
        "2 | aqua | 61.43  \n",
        "  | blue | 61.34 \n",
        "\n",
        "A\n",
        "\n",
        "ubicación | cat | data1 \n",
        "--- | --- | --- \n",
        "1 | blue | 63.48  \n",
        " 1 | green | 62.93  \n",
        "2 | aqua | 61.43  \n",
        "2 | blue | 61.34 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb1m8xtCGtzB"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index()\n",
        "#For more case.pandas methods Week1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CepTIiURm9YX"
      },
      "source": [
        "##**Datos erróneos VS Datos perdidos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr1TTH5DnCTT"
      },
      "source": [
        "Dealing with erroneous values\n",
        "Of course, missing values are not the only problem in datasets – erroneous values are too! So we should look a the other columns with non-null values to see if they could have errors that we can correct. Here, the other columns are business_id, review_id, user_id, and text. However, IDs are arbitrary identifiers which we have no way of ascertaining if they are correct or not, so we have to take the values we are given for granted.\n",
        "\n",
        "text is the text of the user review, but the objective of any attempts to clean this is unclear - what metrics would we use to determine if a review text is \"clean\" or not? The answer to this question is not obvious at all without much more precise direction from the client about what they intend to use this text for, so we will leave it alone for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLRduZ6YnacM"
      },
      "source": [
        "By basic interpretation of what latitude and longitude means, we know that the latitude values should be in the range [-90, 90] and longitude should be in the range [-180, 180]. So we should check that all data points have latitude and longitude values that lie within these ranges. When they are outside the range, we should replace them with acceptable nulls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D67Otkmondm2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The latitude values should be in the range [-90, 90] \n",
        "and longitude should be in the range [-180, 180]. \n",
        "So we should check that all data points have latitude \n",
        "and longitude values that lie within these ranges.\n",
        "\n",
        "\"\"\"\n",
        "df.loc[(businesses['latitude'] < -90) | (df['latitude'] > 90), 'latitude'] = np.nan\n",
        "df.loc[(businesses['longitude'] < -180) | (df['longitude'] > 180), 'longitude'] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4xWz1cM4yLd"
      },
      "source": [
        "###**Separar datos en varias columnas por un carácter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwrvSeEo44UP"
      },
      "outputs": [],
      "source": [
        "split_columns = df[\"variable,variable\"].str.split(pat=\",\", expand=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-ae4eOr5To9"
      },
      "source": [
        "you could use my_string.str.split(pat=\",\", n=1, expand=True) to split only at the first comma and get this result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS3Ss7vR5XHl"
      },
      "outputs": [],
      "source": [
        "split_columns = df[\"variable,variable\"].str.split(pat=\",\", n=1, expand=True)\n",
        "split_columns.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAfDCC_r0lxL"
      },
      "source": [
        "###**Limpieza compleja**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw5B4sES0qPn"
      },
      "source": [
        "####**Fechas**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3cdjdrqpD_d"
      },
      "source": [
        "#####**Poner formato fecha**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoaEEsahgvS0"
      },
      "source": [
        "Use the `!pd.to_datetime()` function with the format=\"%Y%m%d\" argument to process the dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Bw3Cosg6Y_"
      },
      "outputs": [],
      "source": [
        "df[\"variabledate\"] = pd.to_datetime(df[\"variabledate\"], format=\"%Y%m%d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjeulBNXnirn"
      },
      "outputs": [],
      "source": [
        "#convert this index to a date ( dtype='object' to  dtype='datetime64[ns])\n",
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "#To convert this kind of data into a datetime column, we use the pd.to_datetime() \n",
        "#function like this (this function automatically detects the format\n",
        "df[\"VariableDate\"] = pd.to_datetime(df[\"VariableDate\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_i_9wCepbBG"
      },
      "source": [
        "#####**Sacar meses**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0fl_Lq9pkOV"
      },
      "source": [
        "Si se tiene el formto fecha \n",
        "If you want to take the first 4 characters of a string, you can slice it with [0:4]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjBo38sjprcR"
      },
      "source": [
        "Si no se tiene el formato fecha, entonces we had first to convert YYYYMM into a string column because it was originally stored as an integer column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDkCMIw5pt2h"
      },
      "outputs": [],
      "source": [
        "#sobre la misma columna quita los 4 pirmerios númmeros\n",
        "df[\"YYYYMM\"] = df[\"YYYYMM\"].astype(str)\n",
        "df[\"YYYY\"] = df[\"YYYYMM\"].str[0:4] # Notice that we wrote .str before slicing\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So_9eWUIpeBC"
      },
      "outputs": [],
      "source": [
        "#o crea una nueva columna sacando los últimos dos\n",
        "df[\"MM\"] = energy_df[\"YYYYMM\"].str[-2:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWZ3yTugiu1v"
      },
      "source": [
        "Here we used the dt.hour and dt.weekday attributes. These only work with columns that are of type datetime. These basically acess the hour and the weekday of the datetime. (Observe that you have to add dt before you call these attributes.) Other attributes include dt.tz (timezone), dt.year, dt.month, dt.day, dt.minute, and dt.second, amongst others.\n",
        "\n",
        "For more go to https://pandas.pydata.org/docs/reference/series.html#datetimelike-properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTSSZVxupMvZ"
      },
      "source": [
        "####**Interpolación de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwwig9nQV5Q0"
      },
      "source": [
        "**Interpolating missing values**\n",
        "\n",
        "The interpolate() method uses linear interpolation, which is a mathematical method for filling in unknown points based on building a linear regression model on the non-missing points.\n",
        "\n",
        "But userCategory is a string - how can we apply a mathematical model to a string? Luckily, userCategory can only take on two values, so we can convert it to a category type with .astype() and then run the interpolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjtqisdAWIL7"
      },
      "outputs": [],
      "source": [
        "# Convert to a category type\n",
        "df['userCategory'] = df['userCategory'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgkAJL6pWR9L"
      },
      "outputs": [],
      "source": [
        "#After converting userCategory to be a category column, let's see what the data looks like:\n",
        "# The categories\n",
        "df['userCategory'].cat.categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvvb6cEYWj-o"
      },
      "outputs": [],
      "source": [
        "# Let's see the codes assigned to the categories\n",
        "df['userCategory'].cat.codes.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaWd7Zh1WjwQ"
      },
      "source": [
        "You can see there are 2 codes here: 0 and 1, which correspond to the two values ['A'] and ['B']. (The number -1 represents the NaN values.) In order to interpolate the values, we need to call the .interpolate() method after having replaced the -1s with actual null values (np.nans):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ioubv2uUWoyK"
      },
      "outputs": [],
      "source": [
        "# The below code replaces the value -1 with NaN\n",
        "user_cat_codes = df['userCategory'].cat.codes.replace(-1, np.nan)\n",
        "\n",
        "# We now call the interpolate function that actually fills the NaN values with either a 0 or 1\n",
        "user_cat_codes = user_cat_codes.interpolate()\n",
        "user_cat_codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8QKekZJWqY0"
      },
      "outputs": [],
      "source": [
        "#Now we convert the codes to the category data type:\n",
        "user_cat_codes = user_cat_codes.astype(int).astype('category')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TkYKC1YWvF-"
      },
      "source": [
        "Finally, we replace back the 0s and 1s with the actual category names. For this, we can use the .cat.rename_categories() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoKAV9VxWxJb"
      },
      "outputs": [],
      "source": [
        "user_cat_codes = user_cat_codes.cat.rename_categories(df['userCategory'].cat.categories)\n",
        "df['userCategory'] = user_cat_codes\n",
        "df['userCategory']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtogUu0D0kb-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Since the client has indicated they will likely\n",
        " be using the cleaned data for numerous analyses\n",
        "  later on, removing rows with missing values or\n",
        "   even writing in meaningless filler values will\n",
        "   not work. So we need to figure out a sensible\n",
        "    interpolation method.\n",
        "Exercise 1: Given what we know about the data so \n",
        "far, describe a sensible interpolation method that we can use.\n",
        "\"\"\"\n",
        "\n",
        "nan_rows = reviews[reviews['date'].isnull()]\n",
        "date_format = '%Y-%m-%d %H:%M:%S'\n",
        "\n",
        "for index, row in nan_rows.iterrows():\n",
        "    previous_date = reviews.iloc[index - 1]['date']\n",
        "    if isinstance(previous_date, str):\n",
        "        previous_date = datetime.datetime.strptime(previous_date, date_format)\n",
        "    next_date = None\n",
        "    next_date_count = 1\n",
        "    while next_date is None:\n",
        "        try:\n",
        "            next_date = datetime.datetime.strptime(reviews.iloc[index + next_date_count]['date'], date_format)\n",
        "        except Exception:\n",
        "            next_date_count += 1\n",
        "    difference = (next_date - previous_date).seconds\n",
        "    current_date = previous_date + datetime.timedelta(seconds=difference/(next_date_count + 1))\n",
        "    reviews.loc[index, 'date'] = current_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyW9u-C-AP_a"
      },
      "source": [
        "##**Modificar datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgf0qi7LAZS2"
      },
      "source": [
        "###**Estandarizar los carácteres**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehb4mZKDAWgm"
      },
      "outputs": [],
      "source": [
        "#Hacer todas las letras minúsculas\n",
        "df['commodity_transaction'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zPJiYpPAs4F"
      },
      "outputs": [],
      "source": [
        "#Hacer todas las letras minúsculas\n",
        "df['commodity_transaction'].str.upper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "506xz5uWPYnx"
      },
      "source": [
        "###**Variable numérica a categórica**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbyVWQlFPcKS"
      },
      "outputs": [],
      "source": [
        "#Crear las categorías\n",
        "def assign_label(value):\n",
        "    \"\"\"Assign labels to according to their value\n",
        "    \"\"\"\n",
        "    if value < 500:\n",
        "        label = \"Less than 500\"\n",
        "    elif value < 5000:\n",
        "        label = \"Between 500 and 5,000\"\n",
        "    elif value < 50000:\n",
        "        label = \"Between 5,000 and 50,000\"\n",
        "    else:\n",
        "        label = \"More than 50,000\"\n",
        "    return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKHHnVOYPxyf"
      },
      "outputs": [],
      "source": [
        "#Aplicar las categorías a la variable objetivo\n",
        "df[\"numericvarible\"].apply(assign_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xnPznp1zZOO"
      },
      "source": [
        "###**Replace categories by a dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9q1oOilE3Ba"
      },
      "outputs": [],
      "source": [
        "#reemplace un carácter específico por otro (\"este\",\"por este\")\n",
        "df['variable'] = df['variable'].str.lower().str.replace(\"–\", \"-\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-QU1VExzejF"
      },
      "outputs": [],
      "source": [
        "#make a replacement dictionary and rename our categories with this code\n",
        "rename_dict = {\"Priority low\":\"low\",\n",
        "               \"priority_high\":\"high\",\n",
        "               \"priority Medium\":\"medium\",\n",
        "               \"priority_medium\":\"medium\",\n",
        "               \"priority low\":\"low\"\n",
        "              }\n",
        "df[\"tag\"] = df[\"tag\"].replace(rename_dict).astype(\"category\")\n",
        "df[\"tag\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COr3GSfahu05"
      },
      "source": [
        "###**Reemplazar con un diccionario**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3Ot4PjzhUPF"
      },
      "outputs": [],
      "source": [
        "#We can make a replacement dictionary and rename our categories with this code:\n",
        "rename_dict = {\"Priority low\":\"low\",\n",
        "               \"priority_high\":\"high\",\n",
        "               \"priority Medium\":\"medium\",\n",
        "               \"priority_medium\":\"medium\",\n",
        "               \"priority low\":\"low\"\n",
        "              }\n",
        "df[\"tag\"] = df[\"tag\"].replace(rename_dict).astype(\"category\")\n",
        "df[\"tag\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efKF7vQbh1Xn"
      },
      "source": [
        "###**Concatenar para crear valores únicos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu68WfH9h_vN"
      },
      "outputs": [],
      "source": [
        "# Let's generate an id for each trip, in order to uniquely identify each trip. \n",
        "# The trip id can be a combination of start date, end date, and bike id\n",
        "df['trip_id'] = df.apply(lambda x: ':'.join([str(x['Start Date']), str(x['End Date']), str(x['Bike Id'])]), axis=1)\n",
        "\n",
        "# In order for the id to look actually like a unique identifier, let's use base64 encode to convert the id to a base64 string\n",
        "# The command converts the newly created id column into bytes, and then gets the base64 encoded value for the same. \n",
        "# Then the base64 value is converted to string again and then stored in the trip_id column.\n",
        "df['trip_id'] = df['trip_id'].apply(lambda x: base64.b64encode(x.encode()).decode())\n",
        "\n",
        "df['trip_id'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSc60XeQjIj2"
      },
      "source": [
        "###**Redondear datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbmZ4VYEjLHR"
      },
      "outputs": [],
      "source": [
        "#To round a float number up, you can use the .ceil() function from the math module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JIIDvoXRNjt"
      },
      "source": [
        "##**Aplicar una función a varias columnas o filas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBTHLv6dktRV"
      },
      "source": [
        "###**Dividir la información para conocer los bloques de x cantidad**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eValw_ZPk5Hb"
      },
      "source": [
        "How many 30-minute blocks were used up by the rider in each trip (we round up the result with math.ceil() since a fraction of a block counts as an entire block):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut94gRIUk3-q"
      },
      "outputs": [],
      "source": [
        "df['time_blocks'] = df[\"duration_min\"].apply(lambda x: math.ceil(x/30))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjxLhYrUlEPj"
      },
      "source": [
        "###**Sumar columnas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQtUBAV9SKkn"
      },
      "outputs": [],
      "source": [
        "#Dime cuanto suman estas columnas\n",
        "df[[\"v1\", \"v2\", \"v3\", \"v4\", \"v5\"]].apply(sum, axis=\"index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll3-eElEReow"
      },
      "source": [
        "The apply() method has a DataFrame method counterpart that allows you to not only call functions on Series, but on entire DataFrames, either column-wise or row-wise. The usage is very similar:\n",
        "\n",
        "One option\n",
        "my_dataframe.apply(my_custom_function, axis=\"index\")\n",
        "Another option\n",
        "my_dataframe.apply(my_custom_function, axis=\"columns\")\n",
        "The axis parameter, which can take either the value index or the value columns, is what tells pandas to run my_custom_function column-by-column or row-by-row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A6g5DjMSjBU"
      },
      "source": [
        "Write code that returns one number per column in the df_countries DataFrame (but only include the hydro, wind, solar, geothermal, and tide columns). This number should be the sum of all the quantities in each column that are greater than 1,000 units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWF-JG_ISlvA"
      },
      "outputs": [],
      "source": [
        "#Suma los valores de ciertas columnas, pero solo para los valores mayores a 100\n",
        "def sum_greater_10K(this_column):\n",
        "    \"\"\"Sum all the values in `this_column` that are greater than 1,000\n",
        "    \"\"\"\n",
        "    filtered_column = this_column[this_column > 1000]\n",
        "    my_sum = filtered_column.sum()\n",
        "    return my_sum\n",
        "    \n",
        "df_countries[[\"v1\", \"v2\", \"v3\", \"v4\", \"v5\"]].apply(sum_greater_10K, axis=\"index\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbOu-OPilPMP"
      },
      "source": [
        "###**Restar un valor específico a toda la columna**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbB07f_GlSpk"
      },
      "outputs": [],
      "source": [
        "df['variablerestada'] = df['variable'] - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk97CwMGqHEG"
      },
      "source": [
        "###**Contar la cantidad de veces que aparece cada ategoría en una variable/columna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwWt1kPGqHdT"
      },
      "outputs": [],
      "source": [
        "def calculate_review_count(row):\n",
        "    business_id = row['business_id']\n",
        "    business_reviews = reviews[reviews['business_id'] == business_id]\n",
        "    return len(business_reviews)\n",
        "\n",
        "businesses['review_count'] = businesses.apply(calculate_review_count, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F64NHq6dBH9H"
      },
      "source": [
        "##**Análisis de texto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1CBbk6iLGOR"
      },
      "source": [
        "###**Filtrar info**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IJWWFQtLKhb"
      },
      "source": [
        "You can filter a DataFrame with multiple conditions by using the & symbol (similar to when writing conditional if statements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA3Smg5_LnHn"
      },
      "outputs": [],
      "source": [
        "#Dejame ver solo los de la categoría tal, cuyo valor en otra columna sea mayor a 0\n",
        "df[(df.categoricalvariable == category) & (df.numericvariable > 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiM9k-iRMCgb"
      },
      "outputs": [],
      "source": [
        "#Calcula los percentiles y dejame los valores del medio y solo los de la categoría tal\n",
        "#Organizame los valores por otra variablex\n",
        "quantiles = df[\"numericvariable\"].quantile(0.9)\n",
        "df[\n",
        "    (df.numericvariable > quantiles) & (categoricalvariable == category)\n",
        "].sort_values(by=\"variablex\", ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2lbw1PfBL0f"
      },
      "source": [
        "###**Búsqueda de carácteres o palabras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpBsA_W_Buto"
      },
      "source": [
        "For categorical data we first retrieve all the unique values with drop_duplicates() and then add the str.count() string method immediately after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBaTnqeEBQg3"
      },
      "outputs": [],
      "source": [
        "#Quite los dupicados y dígame para las categorías únicas, cuantas \"-\" tienen\n",
        "df[\"commodity_transaction\"].drop_duplicates().str.count(\"-\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgTzavBSCZg9"
      },
      "source": [
        "###**Tabla de frecuencia de un texto por cantidad de varialbes que tienen la misma cantidad de la variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FE-xYfnCxrO"
      },
      "outputs": [],
      "source": [
        "#Digame cuantos carácteres tienen cada cantidad de ese carácter\n",
        "df[\"commodity_transaction\"].drop_duplicates().str.count(\"-\").value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMv6UFwBt0u9"
      },
      "source": [
        "###**Tabla de frecuencias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-6CmFD0EO6R"
      },
      "source": [
        "###**Imprimir todas las categorías que cumplan con una condición**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNOap1biEVHX"
      },
      "outputs": [],
      "source": [
        "#Imprimame todas las categorías que tengan cero \"-\"\n",
        "df[\"variable\"][df[\"variable\"].str.count(\"-\")==0].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROJbmWHtTnMk"
      },
      "source": [
        "#**Crear nueva información dentro del DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNW9MsLXnXVp"
      },
      "outputs": [],
      "source": [
        "#You can easily create a frequency table of the ages present in the dataset with \n",
        "#the .value_counts() method, and report only the 10 most common: \n",
        "df[\"categoric/ordinalvariable\"].value_counts().head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5vtFMdAt8i0"
      },
      "source": [
        "###**Nueva variable resultado de una operación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq7HhoLBTsty"
      },
      "outputs": [],
      "source": [
        "#create a new column with the xname of the symbol and full with \"X\"\n",
        "df[\"xname\"] = \"x\"\n",
        "\n",
        "#Crear una nueva columna con los nombres de cada archivo es últi para unir posteriormente los dataframes\n",
        "df1[\"xname\"] = \"df1\"\n",
        "df2[\"xname\"] = \"df2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLQtuTU_Hgd7"
      },
      "source": [
        "Use the sum() method, which will help us with finding the total production for each row. To sum horizontally (row total, across columns), we use sum(axis=\"columns\"), and to sum vertically (column total, across rows), use sum(axis=\"index\") or simply sum()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG1qdhQ6HnYu"
      },
      "outputs": [],
      "source": [
        "# Here we use the .sum() method. axis=\"columns\" means sum horizontally, not vertically (the default)\n",
        "df[\"variabletotal\"] = df_countries[[\"v1\", \"v2\", \"v3\", \"v3\", \"v4\"]].sum(axis=\"columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77heioEuJQ_"
      },
      "source": [
        "###**Manipular filas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGOurv6XWuCh"
      },
      "outputs": [],
      "source": [
        "#Correr la información una fila hacia arriba o hacia abajo\n",
        "#Hacia arriba\n",
        "df[\"variable\"].head(4).shift(-1)\n",
        "\n",
        "#Hacia abajo\n",
        "df[\"variable\"].head(4).shift(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3BhXy--W-mD"
      },
      "source": [
        "**Original**\n",
        "\n",
        "Date | data\n",
        "--- | ---\n",
        "2014-07-01 | 63.486\n",
        "2014-07-02 | 62.935\n",
        "2014-07-07 | 61.430\n",
        "2014-07-08 | 61.348\n",
        "Name: High, dtype: float64\n",
        "\n",
        "**Hacia arriba**\n",
        "\n",
        "Date | data\n",
        "--- | ---\n",
        "2014-07-01 | 62.935\n",
        "2014-07-03 | 61.430\n",
        "2014-07-07 | 61.348\n",
        "2014-07-08 | NaN\n",
        "Name: High, dtype: float64\n",
        "\n",
        "**Hacia abajo**\n",
        "\n",
        "Date | data\n",
        "--- | ---\n",
        "2014-07-01 | NaN\n",
        "2014-07-02 | 63.486\n",
        "2014-07-03 | 62.935\n",
        "2014-07-08 | 61.430\n",
        "Name: High, dtype: float64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esASKN7DeQoZ"
      },
      "outputs": [],
      "source": [
        "#Crear una nueva columna con resultados de una operación\n",
        "df[\"nuevavariable\"] = (df[\"variable1\"] / df[\"variable2\"]) - 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvFhwUyVnv6b"
      },
      "source": [
        "### **Cruzar datos de dos bases**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TR834akn0-y"
      },
      "outputs": [],
      "source": [
        "#Taer el nombre de un barrio según su ubicación geográfica\n",
        "with open('us-state-shapes.json') as f: states = json.load(f)\n",
        "\n",
        "def get_state_name(row): if not row['latitude'] or not row['longitude']: return None point = Point(row['longitude'], row['latitude']) for state in states['features']: polygon = shape(state['geometry']) if polygon.contains(point): return state['properties']['NAME']\n",
        "\n",
        "businesses['state'] = businesses.apply(get_state_name, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtxoYs8otIaf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmPujOAUFmZA"
      },
      "source": [
        "#**Crear nuevos DataFrame**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GIxGlkTuGDF"
      },
      "source": [
        "###**Unir dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St8jd3OfVdAt"
      },
      "outputs": [],
      "source": [
        "#Stitch all the dataframes together with the pd.concat() function\n",
        "stitchdata = pd.concat([df1, df2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAEnZHjUt8V8"
      },
      "source": [
        "###**Fusionar dataframes por una categoría**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p266YSnOubi7"
      },
      "outputs": [],
      "source": [
        "merged_df = pd.merge(df1, df2, on=\"category\", suffixes=[\"datadf1\", \"datadf2\"])\n",
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WFSIwU1wJgq"
      },
      "source": [
        "####**Merged en detalle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw3qQda6v3Sn"
      },
      "source": [
        "Arguably the most common way of merging DataFrames is using a **left join**, that graphically looks like this:\n",
        "~~~python\n",
        "pd.merge(L, R, on=\"number_plate\", how=\"left\")\n",
        "~~~\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>number_plate</th>      <th>has_tickets</th>      <th>owners_name</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>IBV2750</td>      <td>True</td>      <td>NaN</td>    </tr>    <tr>      <th>1</th>      <td>EUS687</td>      <td>False</td>      <td>Jenny Liu</td>    </tr>    <tr>      <th>2</th>      <td>AYE7756</td>      <td>True</td>      <td>NaN</td>    </tr>  </tbody></table>\n",
        "\n",
        "**Left join**\n",
        "\n",
        "Arguably the most common way of merging DataFrames is using a **left join**, that graphically looks like this:\n",
        "~~~python\n",
        "pd.merge(L, R, on=\"number_plate\", how=\"left\")\n",
        "~~~\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>number_plate</th>      <th>has_tickets</th>      <th>owners_name</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>IBV2750</td>      <td>True</td>      <td>NaN</td>    </tr>    <tr>      <th>1</th>      <td>EUS687</td>      <td>False</td>      <td>Jenny Liu</td>    </tr>    <tr>      <th>2</th>      <td>AYE7756</td>      <td>True</td>      <td>NaN</td>    </tr>  </tbody></table>\n",
        "\n",
        "Here we took all the elements of the `left` DataFrame and tried to find a match in the `right` DataFrame for each one of them. Then we reported all the elements of the `left` DataFrame regardless of whether we did find a match or not. In this case, the only car that had a match was `EUS687` and hence it is the only one that has an associated `owners_name` in the result.\n",
        "\n",
        "**Right join**\n",
        "\n",
        "This is exactly the same as the left join, with the difference that we report all the elements of the `right` DataFrame instead of those of the `left` DataFrame:\n",
        "\n",
        "~~~python\n",
        "pd.merge(L, R, on=\"number_plate\", how=\"right\")\n",
        "~~~\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>number_plate</th>      <th>has_tickets</th>      <th>owners_name</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>6TRJ244</td>      <td>NaN</td>      <td>Michael Holmes</td>    </tr>    <tr>      <th>1</th>      <td>EUS687</td>      <td>False</td>      <td>Jenny Liu</td>    </tr>    <tr>      <th>2</th>      <td>532484</td>      <td>NaN</td>      <td>Geoffrey Frank</td>    </tr>  </tbody></table>\n",
        "\n",
        "This would be equivalent to `pd.merge(R, L, on=\"number_plate\", how=\"left\")` (notice that we swapped the DataFrames).\n",
        "\n",
        "**Inner join**\n",
        "\n",
        "This kind of join only reports those elements that are both in the `right` and the `left` DataFrames:\n",
        "\n",
        "~~~python\n",
        "pd.merge(L, R, on=\"number_plate\", how=\"inner\")\n",
        "~~~\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>number_plate</th>      <th>has_tickets</th>      <th>owners_name</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>EUS687</td>      <td>False</td>      <td>Jenny Liu</td>    </tr>  </tbody></table>\n",
        "\n",
        "**Full join**\n",
        "\n",
        "Finally, a **full join** reports all the elements of both DataFrames, including those that had a match and those that did not:\n",
        "\n",
        "~~~python\n",
        "pd.merge(L, R, on=\"number_plate\", how=\"outer\")\n",
        "~~~\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>number_plate</th>      <th>has_tickets</th>      <th>owners_name</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>IBV2750</td>      <td>True</td>      <td>NaN</td>    </tr>    <tr>      <th>1</th>      <td>EUS687</td>      <td>False</td>      <td>Jenny Liu</td>    </tr>    <tr>      <th>2</th>      <td>AYE7756</td>      <td>True</td>      <td>NaN</td>    </tr>    <tr>      <th>3</th>      <td>6TRJ244</td>      <td>NaN</td>      <td>Michael Holmes</td>    </tr>    <tr>      <th>4</th>      <td>532484</td>      <td>NaN</td>      <td>Geoffrey Frank</td>    </tr>  </tbody></table>\n",
        "\n",
        "Keep in mind that the keyword for a full join in `pandas` is not ~`full`~ but `outer`. This is because \"full join\" is a shortened form of \"full outer join\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBcxM8diTTg"
      },
      "source": [
        "For more go to Week 3 case.data_cleaning_basics.coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC-kav3dHb8h"
      },
      "source": [
        "###**Extraer información por una categoría**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcHbntmHHjol"
      },
      "outputs": [],
      "source": [
        "#Saqueme todas las filas que contengan una categoría específica en una variable particular\n",
        "df[df[\"variable\"] == \"categoría\"]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr0OYxFrGGhJ"
      },
      "source": [
        "###**Extraer información por una condición**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSt1EM7BwCmj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNg0oZwNGMzi"
      },
      "outputs": [],
      "source": [
        "#Saqueme todas las filas que contengan una palabra específica en una variable particular\n",
        "dfextraid=df[\"variable\"][df[\"variable\"].str.contains(\"palabrade interes\")].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuiVP5EQtTRI"
      },
      "source": [
        "###**Crear nuevas tablas con datos espefíficos**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMj3-_ZwtYJe"
      },
      "outputs": [],
      "source": [
        "# Filtering the DataFrames\n",
        "nuevatabla = df[df[\"variablecategorica\"] == \"Categoría\"]\n",
        "\n",
        "# Getting rid of unnecesary columns\n",
        "nuevatabla = nuevatabla.drop(columns=[\"Columnaborrar1\", \"Columnaborrar2\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAnYUEaETvDp"
      },
      "source": [
        "###**Tablas dinámicas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWJ7ouHeTz5G"
      },
      "source": [
        "Se tiene una variable (columna) con varias categorías con otra información asociada\n",
        "\n",
        "ubicación | cat | data1 \n",
        "--- | --- | --- \n",
        "1 | blue | 63.48  \n",
        "1 | green | 62.93  \n",
        "2 | aqua | 61.43  \n",
        "2 | blue | 61.34 \n",
        "\n",
        "Y quiero crear una pivot para volver colmnas la variable cat\n",
        "\n",
        "ubicación | blue | green | aqua\n",
        "--- | --- | --- | ---   \n",
        "1 |  63.48 |62.93 | nan\n",
        "2 | 61.34 | nan | 61.43\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBZifYzIDeSa"
      },
      "source": [
        "The first thing we do is to turn the values in the commodity transaction column into our new column names and keep only the quantity column as the new values. We define the keep_values list to include only the values that we are interested in converting into columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY1bx7dtDgqg"
      },
      "outputs": [],
      "source": [
        "keep_values =  [\n",
        "        \"Electricity - Gross demand\",\n",
        "        \"Electricity - Gross production\",\n",
        "        \"Electricity - imports\",\n",
        "        \"Electricity - exports\",\n",
        "        \"Electricity - total hydro production\",\n",
        "        \"Electricity - total wind production\",\n",
        "        \"Electricity - total solar production\",\n",
        "        \"Electricity - total geothermal production\",\n",
        "        \"Electricity - total tide, wave production\",\n",
        "]\n",
        "\n",
        "# Filtering to keep only electricity energies\n",
        "df_filtered = df[df[\"commodity_transaction\"].isin(keep_values)]\n",
        "\n",
        "df_countries = pd.pivot_table(\n",
        "    df_filtered,\n",
        "    values=\"quantity\",\n",
        "    index=[\"country_or_area\", \"year\"],\n",
        "    columns=\"commodity_transaction\",\n",
        ")\n",
        "\n",
        "df_countries\n",
        "\n",
        "#For more case.pandas methods Week1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neLC1wXAN689"
      },
      "outputs": [],
      "source": [
        "nd index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h4T6ZokNgFS"
      },
      "source": [
        "Get a DataFrame with only the categoricalvariable category1 and category2 values kept numeric variablex, and as columns and indexed by another categorical variablec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWSwBKZZNeAE"
      },
      "outputs": [],
      "source": [
        "newpivot = pd.pivot_table(\n",
        "    df, values=\"variablex\", index=[\"variablec\"], columns=\"categoricalvariable\",\n",
        ").reset_index()[[\"variablec\", category1, category2]]\n",
        "\n",
        "newpivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgQRhjHIHCmI"
      },
      "source": [
        "#**Exploratory Data Analysis - EDA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJRItBzXwCjb"
      },
      "source": [
        "##**Descriptivos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgovlJSuglOt"
      },
      "source": [
        "###**Medidas de tendencia central**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVPK-Q4WwHiJ"
      },
      "outputs": [],
      "source": [
        "#Sacar la media de una columna \n",
        "df[\"nombredelacolumna\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7vu_b5rwm9m"
      },
      "outputs": [],
      "source": [
        "#Sacar la mediana de una columna \n",
        "df[\"nombredelacolumna\"].median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KSgtc6JwqGE"
      },
      "outputs": [],
      "source": [
        "#Sacar la moda de una columna \n",
        "df[\"nombredelacolumna\"].mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2ZBdPe4wxaC"
      },
      "outputs": [],
      "source": [
        "#Sacar dato de la media por agrupación de información categórica de otra columna\n",
        "df[[\"categorias\",\"datos\"]].groupby(\"categorías\").mean()\n",
        "#o\n",
        "#Sacar dato media para varias categorias\n",
        "df.groupby(['categorias'])[\"datos\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19ZaxnxaxJXS"
      },
      "outputs": [],
      "source": [
        "#Crear información descriptiva a partir de la información agrupada\n",
        "medias_agrupadas = df[[\"variablecategorica1\",\"datos\"]].groupby(\"variablecategorica1\").mean()\n",
        "\n",
        "#Cambiar normbre a las categorías\n",
        "mediacategoría1v1 = (medias_agrupadas.loc[\"categoria1variable1\"])[0]\n",
        "mediacategoria2v1 = (medias_agrupadas.loc[\"categoria2variable1\"])[0]\n",
        "\n",
        "#Sacar el porcentaje que representa la categoría 2\n",
        "percent = (mediacategoría1v1-mediacategoria2v1)/mediacategoria2v1 \n",
        "\n",
        "#Imprimir un texto con los resultados ({:.2f} pide el resultado con dos decimales)\n",
        "#llamando la función .format(percent*100)) al final.\n",
        "\n",
        "print(\"En promedio la categoría 2 representa el {:.2f}% de los datos\".format(percent*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWvKuNjTXwH3"
      },
      "outputs": [],
      "source": [
        "#Imprimir las medidas de tendencia central de los datos para una categoría específica de la varcat1\n",
        "mean = df[df[\"varcat1\"] == \"categ1\"][\"datos\"].mean()\n",
        "median = df[df[\"varcat1\"] == \"categ1\"][\"datos\"].median()\n",
        "mode = df[df[\"varcat1\"] == \"categ1\"][\"datos\"].mode()\n",
        "mode = mode.values[0]\n",
        "\n",
        "print(\n",
        "    \"The mean, median and mode are: {:.2f}, {:.2f}, {}.\".format(\n",
        "        mean, median, mode\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxQ-VBAIarQc"
      },
      "outputs": [],
      "source": [
        "# sacar la media de los datos para una categoría específica de la varcat1\n",
        "datosdelavarcat1 = df[df[\"varcat1\"] == \"categoria1varcat1\"][\"datos\"]\n",
        "media = datosdelavarcat1.mean()\n",
        "desvest = datosdelavarcat1.std()\n",
        "\n",
        "print(\"Mean: {:.2f}\\nStd: {:.2f}\\n\".format(media, desvest))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d8y8smX3SJ2"
      },
      "outputs": [],
      "source": [
        "#Mostrar media, DS, min, max & quartiles organizados por categorías de una variable\n",
        "variablecategorica_datos = df[[\"datos\", \"variablecategorica\"]].groupby(\"variablecategorica\")\n",
        "variablecategorica_datos.describe()\n",
        "#o\n",
        "df.groupby(['variablecategorica'])[\"datos\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGf6QTMKhA-S"
      },
      "source": [
        "###**Dispersión y cuartiles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7wpInRI_toI"
      },
      "outputs": [],
      "source": [
        "#imprimir minimos y máximos\n",
        "print(df[\"variable\"].min())\n",
        "print(df[\"variable\"].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkDGRSfF2DFD"
      },
      "outputs": [],
      "source": [
        "#Sacar mínimo, máximo y mostrar el rango con agrupación por categorías\n",
        "min = df[df[\"variable1\"] == \"categoria1v1\"][\"datos\"].min()\n",
        "max = df[df[\"variable1\"] == \"categoria1v1\"][\"datos\"].max()\n",
        "rnge = max - min\n",
        "print(\"El rango de los datos agrupados por la variable 1 va desde: [${:.2f} , ${:.2f}]\".format(min, max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsLdtpmuhrEW"
      },
      "outputs": [],
      "source": [
        "#desvest\n",
        "df[\"nombredelacolumna\"].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htt0xrdW2_Bq"
      },
      "outputs": [],
      "source": [
        "# Interquartile range\n",
        "nombredatos = df[df[\"variable1\"] == \"categoria1v1\"][\"datos\"]\n",
        "iqr = nombredatos.quantile(q=0.75) - nombredatos.quantile(q=0.25)\n",
        "print(\n",
        "    \"The nombredatos for the middle 50% of variable1 range from ${:.2f} to ${:.2f}.\\n\\nThis is an interquartile range of ${:.2f}.\".format(\n",
        "        nombredatos.quantile(q=0.25), nombredatos.quantile(q=0.75), iqr\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PunIYRO0kTJZ"
      },
      "outputs": [],
      "source": [
        "### Percentiles\n",
        "perc0 = df[\"variable\"].quantile(0)\n",
        "perc25 = df[\"variable\"].quantile(0.25)\n",
        "perc50 = df[\"variable\"].quantile(0.50)\n",
        "perc75 = df[\"variable\"].quantile(0.75)\n",
        "perc100 = df[\"variable\"].quantile(1)\n",
        "\n",
        "print(per0)\n",
        "print(per25)\n",
        "print(per50)\n",
        "print(per75)\n",
        "print(per100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o8dwNGjSPav"
      },
      "source": [
        "###**Porcentajes de variables categóricas cruzadas**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h-ZqmuFLqse"
      },
      "outputs": [],
      "source": [
        "# Probabilidad de que siendo una categoría específica para la variable categorica 1, sea también una categoría de la variable cateogirca 2\n",
        "\n",
        "#Cuente los regristros de la categoría especifica vcat1 que se tengan en la variablecategorica2\n",
        "contarregistroscruzados = df[df[\"variablecategorica1\"] == \"categoriaespecifica\"][\"variablecategorica2\"].value_counts()\n",
        "#de los registros cruzados, digame cuantos con de la categoría x en la vcat2, y dividalo por el total de registros\n",
        "p = contarregistroscruzados[\"categoriaxvcat2\"] / (contarregistroscruzados.sum())\n",
        "print(p * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxhnlVlcPxfA"
      },
      "outputs": [],
      "source": [
        "# Probabilidad de que, entre todos los datos, ser de una categoría específica para la variablecat1 y una categoría de la variablecat2\n",
        "\n",
        "#Cuente los regristros de la categoría especifica vcat1 que se tengan en la variablecategorica2\n",
        "contarregistroscruzados = df[df[\"variablecategorica1\"] == \"categoriaespecifica\"][\"variablecategorica2\"].value_counts()\n",
        "\n",
        "#tome esos registros y dividalos por el tamaño total de la muestra contando todas las filas\n",
        "p = contarregistroscruzados[\"variablecategorica1\"]/df.shape[0]\n",
        "\n",
        "print(\"{:.2f}\".format(p * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utvDfaPCwtvN"
      },
      "source": [
        "##**Supuestos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIuDZZx2fiWF"
      },
      "source": [
        "###**Área bajo la curva en una distribución normal**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM5ZZd7nifWo"
      },
      "source": [
        "An ideal normal distribution\n",
        "* Approximately 68% of samples in a normal distribution fall within one standard deviation of the mean\n",
        "* Approximately 95% of samples fall within two standard deviations of the mean\n",
        "* Approximately 99.7% of samples fall within three standard deviations of the mean\n",
        "\n",
        "It is common practice to refer to a data point's number of standard deviations from the mean as its **$z$-score**\n",
        "\n",
        "$$z-score = (x - mean) / desvest$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYbrbrUXeXRE"
      },
      "outputs": [],
      "source": [
        "print(\"One std above the mean: {:.1f}\".format(media + desvest))\n",
        "# Probability of data have a value>x assuming it have a normal distribution\n",
        "#transformar a z\n",
        "P = 0.68 / 2 + 0.5\n",
        "print(\"{:.2f}\".format(P))\n",
        "print(\"P > 37 = {:.2f} \".format((1 - P) * 100))\n",
        "\n",
        "# El 95% de las observaciónes están a 2 desvest de la media\n",
        "lower_lim = media - 2 * desvest\n",
        "upper_lim = media + 2 * desvest\n",
        "\n",
        "print(\"El 95% de los datos es < {:.2f} y > {:.2f}\".format(lower_lim, upper_lim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AugN0XVyqBW"
      },
      "source": [
        "###**Normalidad**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igaEX-XVyvvg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Normalidad de los residuos Shapiro-Wilk test\n",
        "shapiro_test = stats.shapiro(df.variable)\n",
        "print(f\"Variable name: {shapiro_test}\", p-value = {p_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLgElEB7zC3E"
      },
      "outputs": [],
      "source": [
        "# Normalidad de los residuos D'Agostino's K-squared test\n",
        "k2, p_value = stats.normaltest(df.variable)\n",
        "print(f\"Variable name: Estadístico = {k2}, p-value = {p_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAGQ0ekUz1AE"
      },
      "source": [
        "###**Transformación a log natural**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCft9ePOzSnc"
      },
      "outputs": [],
      "source": [
        "# Transformación logarítmica de los datos\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
        "\n",
        "sm.qqplot(\n",
        "    np.log(df.variable),\n",
        "    fit   = True,\n",
        "    line  = 'q',\n",
        "    alpha = 0.4,\n",
        "    lw    = 2,\n",
        "    ax    = ax\n",
        ")\n",
        "ax.set_title('Gráfico Q-Q log(height)', fontsize = 13)\n",
        "ax.tick_params(labelsize = 7)\n",
        "\n",
        "\n",
        "shapiro_test = stats.shapiro(np.log(df.variable))\n",
        "print(f\"Variable name: {shapiro_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi4Kw5KIzxP9"
      },
      "source": [
        "###**Homocedasticidad**\n",
        "Pendiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTnLMJaiPKer"
      },
      "source": [
        "#**Gráficos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jb9wSdAUEYF"
      },
      "source": [
        "##**Histogramas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WHVPWHvvTg"
      },
      "source": [
        "###**Visualizar su distribución simple**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO-LAGb9PsYx"
      },
      "outputs": [],
      "source": [
        "#Histograma simple\n",
        "df[\"varaible\"].plot.hist(title=\"varialbe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYBTHZ35uap9"
      },
      "outputs": [],
      "source": [
        "# Gráfico distribución variables\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "\n",
        "axs[0].hist(x=df.nombrevariable, bins=20, color=\"#3182bd\", alpha=0.5)\n",
        "axs[0].plot(df.nombrevariable, np.full_like(df.nombrevariable, -0.01), '|k', markeredgewidth=1)\n",
        "axs[0].set_title('Distribución nombrevariable')\n",
        "axs[0].set_xlabel('nombrevariable')\n",
        "axs[0].set_ylabel('counts')\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEKIqi5YjJNg"
      },
      "outputs": [],
      "source": [
        "#Histograma\n",
        "\n",
        "dataforhistogram = df['variable'].plot.hist()\n",
        "\n",
        "hvm.set_title(\"Title\")\n",
        "hvm.set_xlabel(\"unidades\")\n",
        "\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDNRYHqQPbel"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEo6BY9-QD4v"
      },
      "source": [
        "###**Distribución con categorías de otra variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX0wQvxwQJ7X"
      },
      "outputs": [],
      "source": [
        "#Using seaborn, we can create box plots in which the data are grouped by a second column\n",
        "sns.boxplot(x = \"categoricalvariable\", y='numericalvariable', data = listings)\n",
        "plt.title(\"Boxplot of numervical vs. categorical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2mekSU4UPUQ"
      },
      "source": [
        "##**Comparación de variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofnQgq-R01G0"
      },
      "source": [
        "###**Scatter plot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y_lEqkSuzeQ"
      },
      "source": [
        "Al tener una tabla con los datos a comparar en paralelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbD2f_tLuvnP"
      },
      "outputs": [],
      "source": [
        "plt.scatter(x = merged_df[\"variable1\"], y = merged_df[\"variable2\"]) # The plot\n",
        "plt.title(\"Title\") # Adding a title\n",
        "plt.xlabel(\"variable1\") # Adding axis labels\n",
        "plt.ylabel(\"variable2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhRFRBVCK8Ao"
      },
      "outputs": [],
      "source": [
        "#Crear un scatter plot con  Plotly Express para ver datos al pasar sobre un punto de la gráfica\n",
        "px.scatter(df, x=\"variablenumerica1\", y=\"variablenumerica2\", color=\"Variablecategorica1\")\n",
        "#Para ver aún más infomación sobre un punto de la gráfica\n",
        "px.scatter(df,\n",
        "           x=\"variablenumerica1\",\n",
        "           y=\"variablenumerica2\",\n",
        "           color=\"Variablecategorica1\",\n",
        "           hover_data=['Variablecategorica2', 'Variablecategorica2', 'Variablecategorica3', 'Variablecategorica4'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiE1jcx004XS"
      },
      "source": [
        "###**Boxplot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFwUUNt106nl"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=\"categoricalvariable\", y=\"numericvariable\", data=df)\n",
        "plt.xticks(rotation=90)\n",
        "#Or\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "sns.boxplot(x=\"categoricalvariable\", y=\"numericvariable\", data=df, ax=ax)\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCpHZWQB1VMs"
      },
      "source": [
        "También se puede sacar in strip plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNtPfZNR1Y5U"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "sns.stripplot(x=\"Description\", y=\"Value\", data=energy_df, ax=ax)\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqZ1iZwJ1jGA"
      },
      "source": [
        "\n",
        "\n",
        "![stripplot](https://seaborn.pydata.org/_images/seaborn-stripplot-3.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH-B5w4KcLSH"
      },
      "source": [
        "##**Density plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUyYTahHcYJy"
      },
      "outputs": [],
      "source": [
        "#Crea un plot para cada variable categórica\n",
        "for newnameplot in neigh_to_look:\n",
        "data_plot = df[df['categoricalvariable']]\n",
        "sns.kdeplot(data=data_plot,x='variable1', hue='variable2', shade = True, label=\"variable2\")\n",
        "plt.title(\"variable2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VU2NkYtcY4-"
      },
      "outputs": [],
      "source": [
        "#Crea un plot para categorías específicas\n",
        "listadecategorias = ['cat1','cat2','c3']\n",
        "\n",
        "for newnameplot in listadecategorias:\n",
        "    data_plot = df[df['categoricalvariable'] == newnameplot]\n",
        "    sns.kdeplot(data=data_plot,x='variable1', hue='variable2', shade = True, label=\"variable2\")\n",
        "    plt.title(\"variable2 in \" + newnameplot)\n",
        "    plt.show()\n",
        "\n",
        "    #La línea  plt.title(\"variable2 in \" + newnameplot) personaliza el nombre \n",
        "    #de la gráfica dependiendo de la categoría "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jEGuhZJwJ4w"
      },
      "source": [
        "###**Gráfico de disepersión scatterplot**\n",
        "Verificar que haya una relación visible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6WbIscswUYX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
        "ax.scatter(x=df.bmi, y=df.charges, alpha= 0.8)\n",
        "ax.set_xlabel('variable1')\n",
        "ax.set_ylabel('variable2');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtVwhoM-0838"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlr3MGYEaIDC"
      },
      "source": [
        "###**Gradiente de colores para tabla de correlación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntrjh-edaIme"
      },
      "outputs": [],
      "source": [
        "#Sacar matriz de correlación con un gradiente de colores\n",
        "corr = central.corr()\n",
        "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwLe5BJ-VuX6"
      },
      "source": [
        " #### [**Choosing Colormaps in Matplotlib**](https://matplotlib.org/stable/tutorials/colors/colormaps.html/).\n",
        "\n",
        "\n",
        "\n",
        "For the Diverging maps, we want to have monotonically increasing  values up to a maximum, which should be close to , followed by monotonically decreasing  values. We are looking for approximately equal minimum  values at opposite ends of the colormap. By these measures, BrBG and RdBu are good options. coolwarm is a good option, but it doesn't span a wide range of  values (see grayscale section below).\n",
        "\n",
        "![Python logo](https://matplotlib.org/stable/_images/sphx_glr_colormaps_004_2_0x.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtGf203et9xW"
      },
      "outputs": [],
      "source": [
        "#Esto no es un código, es un separador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5thVXMYOWAdV"
      },
      "outputs": [],
      "source": [
        "#Pendiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhnCuosFz3h"
      },
      "source": [
        "##**Series de tiempo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63cI7CuFq62F"
      },
      "outputs": [],
      "source": [
        "#Hacer que la tabla sea indexada por fecha\n",
        "df.index= df.set_index('fecha') \n",
        "#Asegurarse que fecha tenga el formato fecha\n",
        "df.index = pd.to_datetime(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDWvYTdtr4Pa"
      },
      "outputs": [],
      "source": [
        "#Plot one series by categorical variable\n",
        "df.groupby(\"variablecategorica1\")[\"variablecontinua\"].plot(legend=True,\n",
        "                                         title=\"Titulografica\",\n",
        "                                         figsize=(15,6)\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrAcNM8gs0rn"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://natmeurer.com/content/images/2019/11/big.png\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNQGCTehxL4I"
      },
      "source": [
        "También se puede usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuuTbQ0QxOSo"
      },
      "outputs": [],
      "source": [
        "# Filtering the DataFrame\n",
        "timeseriesdf = df[df[\"Categoryvariable\"] == \"Category\"].copy()\n",
        "\n",
        "plt.figure(figsize=(15, 3)) # To make it larger\n",
        "plt.plot(timeseriesdf[\"Date\"], timeseriesdf[\"variable\"]) # The order is plt.plot(x,y)\n",
        "plt.title(\"Title\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjSArNk2yw5t"
      },
      "outputs": [],
      "source": [
        "#We can even make a plot of percentage variations\n",
        "\n",
        "plt.figure(figsize=(15, 3)) # To make it larger\n",
        "plt.plot(timeseriesdf[\"Date\"], timeseriesdf[\"variable\"].pct_change()) # The order is plt.plot(x,y)\n",
        "plt.title(\"Title\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP_jr6ZPV9gG"
      },
      "source": [
        "##**Heat maps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsnnWzOUzkTp"
      },
      "source": [
        "1. first create a pivot table as usual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qsfh1ekOzjvN"
      },
      "outputs": [],
      "source": [
        "#Saca los datos de la categoría excogida, po meses en y años en x\n",
        "heatmapdf = df[df[\"Variablecategorica\"] == \"Categorica\"]\n",
        "pivoted_df = pd.pivot_table(data=generate_df, index=\"MM\", columns=\"YYYY\", values=\"Value\")\n",
        "pivoted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a1Dty5B0Mv8"
      },
      "source": [
        "2. Now it's simply a matter of using sns.heatmap(). You can customize the plot (like setting its size and adding a title and axis labels) using matplotlib just as with the previous plots, because seaborn is built on top of matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXzO33ga0O90"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 3)) # We can set the size using matplotlib\n",
        "sns.heatmap(pivoted_df, cmap=\"coolwarm\") # cmap defines the color palette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yd3USyT0bGE"
      },
      "source": [
        "Result:\n",
        "\n",
        "![Heatmap](https://i.stack.imgur.com/dg0Nx.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12zcxoaUXDE"
      },
      "source": [
        "##**Datos geográficos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-8cwIJcA0BN"
      },
      "source": [
        "####**Definir Ubicación geográfica**\n",
        "with open('us-state-shapes.json') as f:\n",
        "    states = json.load(f)\n",
        "    \n",
        "def get_state_name(row):\n",
        "    if not row['latitude'] or not row['longitude']:\n",
        "        return None\n",
        "    point = Point(row['longitude'], row['latitude'])\n",
        "    for state in states['features']:\n",
        "        polygon = shape(state['geometry'])\n",
        "        if polygon.contains(point):\n",
        "            return state['properties']['NAME']\n",
        "        \n",
        "businesses['state'] = businesses.apply(get_state_name, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_N0UBlvpI2r"
      },
      "source": [
        "We must find the county and state each business belongs in and add this to the table. We use the geoJSON files that contain the geoshapes of each state and county in the US. We will be using the shapely library to figure out whether a point is present inside the shape that represents the border of each state/county:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1wtsmuppOCZ"
      },
      "outputs": [],
      "source": [
        "with open('us-state-shapes.json') as f:\n",
        "    states = json.load(f)\n",
        "    \n",
        "def get_state_name(row):\n",
        "    if not row['latitude'] or not row['longitude']:\n",
        "        return None\n",
        "    point = Point(row['longitude'], row['latitude'])\n",
        "    for state in states['features']:\n",
        "        polygon = shape(state['geometry'])\n",
        "        if polygon.contains(point):\n",
        "            return state['properties']['NAME']\n",
        "        \n",
        "businesses['state'] = businesses.apply(get_state_name, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROTQZ-rZpSSQ"
      },
      "source": [
        "Now find the county of each business and add them to the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifqXt6CCpRIu"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error # Skip cell execution, remove this line to execute.\n",
        "# Given the size of the file us-county-shapes.json, it is recommended to skip executing this cell during lecture time.\n",
        "with open('us-county-shapes.json') as f:\n",
        "    counties = json.load(f)\n",
        "    \n",
        "def get_county_name(row):\n",
        "    if not row['latitude'] or not row['longitude']:\n",
        "        return None\n",
        "    point = Point(row['longitude'], row['latitude'])\n",
        "    for county in counties['features']:\n",
        "        polygon = shape(county['geometry'])\n",
        "        if polygon.contains(point):\n",
        "            return county['properties']['NAME']\n",
        "        \n",
        "businesses['county'] = businesses.apply(get_county_name, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrp3XRWWUdJ8"
      },
      "source": [
        "###**Create an interactive map**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fmJVJr4UePs"
      },
      "outputs": [],
      "source": [
        "#Create an interactive map\n",
        "folium_map = folium.Map()\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KshAXli_Uvsp"
      },
      "outputs": [],
      "source": [
        "#set that as the center of our map with coordinades of new york\n",
        "ny_coords = [40.738, -73.98] # lat, long\n",
        "folium_map = folium.Map(location=ny_coords)\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCnkANgXU7ek"
      },
      "outputs": [],
      "source": [
        "#set a default zoom that gives a closer view of the city\n",
        "ny_coords = [40.738, -73.98] # lat, long\n",
        "folium_map = folium.Map(location=ny_coords, zoom_start=13)\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ9DqwmgVDi0"
      },
      "outputs": [],
      "source": [
        "#change the title (the default is OpenStreetMap)\n",
        "ny_coords = [40.738, -73.98] # lat, long\n",
        "folium_map = folium.Map(location=ny_coords, zoom_start=13, tiles=\"OpenStreetMap\")\n",
        "folium_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps0OzQa3VMHL"
      },
      "source": [
        "There are other styles available for the tiles argument:\n",
        "```\n",
        "Stamen Toner \n",
        "Stamen Terrain\n",
        "Stamen Watercolor\n",
        "CartoDB positron\n",
        "CartoDB dark_matter\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZR_pjPfWbpV"
      },
      "source": [
        "###**Heat map**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQnwTBXSXPOu"
      },
      "source": [
        "1. Create the canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFQd3g7JWM6r"
      },
      "outputs": [],
      "source": [
        "#1Create the canvas\n",
        "folium_hmap = folium.Map(location=ny_coords, zoom_start=13, tiles=\"OpenStreetMap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfNlF5nIWsDM"
      },
      "source": [
        "2. Prepare the data. folium needs a list in which each element contains the latitude, the longitude, and the price of the listing. We can use Python's handy zip() function, which takes two iterables and matches their elements one-by-one pairwise.\n",
        "\n",
        "**Note**: In order to inspect the elements inside a zip object, we first need to convert it into a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XakVTWj-XCwh"
      },
      "outputs": [],
      "source": [
        "my_zip = zip(df['latitude'], df['longitude'], df['price'])\n",
        "list_of_my_zip = list(my_zip)\n",
        "list_of_my_zip[0:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6p6TNqlXMXx"
      },
      "source": [
        "3. The next step is to create a HeatMap layer with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0m3aht1XTER"
      },
      "outputs": [],
      "source": [
        "hm_layer = HeatMap(list_of_my_zip,\n",
        "                   # These are parameters that we tweak manually to adjust color\n",
        "                   # See folium docs for more information\n",
        "                   min_opacity=0.2,\n",
        "                   radius=8,\n",
        "                   blur=6, \n",
        "                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAam2w9hXVqR"
      },
      "source": [
        "4. We can finally add this layer to our map and see the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJlNCE77XZDI"
      },
      "outputs": [],
      "source": [
        "folium_hmap.add_child(hm_layer)\n",
        "folium_hmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiUte41QX19A"
      },
      "source": [
        "5. Let's save the map as HTML, so that we can share it later with people who don't have Jupyter in their computers. As HTML files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8Q0nDZEXxEv"
      },
      "outputs": [],
      "source": [
        "folium_hmap.save(\"hmap.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EeIyUkzYYny"
      },
      "source": [
        "###**Mark points in the map by catogory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_9ymQp5YhK5"
      },
      "outputs": [],
      "source": [
        "#Determinar la ubicación por categoría\n",
        "mapcategory1 = df.loc[ df['variable']==category1, [\"latitude\",\"longitude\" ] ]\n",
        "mapcategori2 = df.loc[ df['variable']==category2, [\"latitude\",\"longitude\" ] ]\n",
        "#Crear lienzo\n",
        "folium_map = folium.Map(location=[40.738, -73.98],\n",
        "                        zoom_start=13,\n",
        "                        tiles=\"OpenStreetMap\")\n",
        "#Poner marcas en el mapa\n",
        "for i in range(1000):\n",
        "    marker = folium.CircleMarker(location=[mapcategory1[\"latitude\"].iloc[i],mapcategory1[\"longitude\"].iloc[i]],radius=5,color=\"blue\",fill=True)\n",
        "    marker.add_to(folium_map)\n",
        "\n",
        "for i in range(1000):\n",
        "    marker = folium.CircleMarker(location=[mapcategori2[\"latitude\"].iloc[i],mapcategori2[\"longitude\"].iloc[i]],radius=5,color=\"red\",fill=True)\n",
        "    marker.add_to(folium_map)    \n",
        "    \n",
        "folium_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "317Ps8AsR6se"
      },
      "source": [
        "#**Estadistica correlacional**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MuiTQVM09Wc"
      },
      "source": [
        "###**Correlación simple**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RivFukhvr0c"
      },
      "source": [
        "Use pandas and the .corr() method to compute a correlation matrix to compare correlations between variables\n",
        "\n",
        "\n",
        "```python\n",
        "DataFrame.corr(method='pearson', min_periods=1)[source]\n",
        "```\n",
        "\n",
        "Compute pairwise correlation of columns, excluding NA/null values.\n",
        "\n",
        "*Parameters method*\n",
        "* *{‘pearson’, ‘kendall’, ‘spearman’}*\n",
        "\n",
        "*min_periodsint*\n",
        "* Optional Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCJp0HA8QyRf"
      },
      "outputs": [],
      "source": [
        "##Correlación simple that gives you a correlation matrix\n",
        "df[[\"varialble1\", \"variable2\"]].corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BLchxAoR55U"
      },
      "outputs": [],
      "source": [
        "#This would be the correlation matrix for all the variables in the DataFrame\n",
        "corrm = df.corr()\n",
        "corrm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4Of6XVT0OXl"
      },
      "outputs": [],
      "source": [
        "#Cálculo de correlación con Pandas\n",
        "print('Correlación Pearson: ', df['variablecontinua1'].corr(df['variablecontinua2'], method='pearson'))\n",
        "print('Correlación spearman: ', df['variablecontinua1'].corr(df['variablecontinua2'], method='spearman'))\n",
        "print('Correlación kendall: ', df['variablecontinua1'].corr(df['variablecontinua2'], method='kendall'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou21kZbfHRQC"
      },
      "source": [
        "La implementación de Scypy.stats sí permite calcular la significancia estadística además del coeficiente de correlación. La función stats.pearsonr(), devuelve un error si alguna de las observaciones contienen valores NA/null. Las funciones stats.spearmanr() y stats.kendalltau() sí permiten excluirlos de forma automática si se indica nan_policy='omit'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zby86EVcHX7k"
      },
      "outputs": [],
      "source": [
        "# Cálculo de correlación y significancia con Scipy\n",
        "r, p = stats.pearsonr(df['variablecontinua1'], df['variablecontinua2'])\n",
        "print(f\"Correlación Pearson: r={r}, p-value={p}\")\n",
        "\n",
        "r, p = stats.spearmanr(df['variablecontinua1'], df['variablecontinua2'])\n",
        "print(f\"Correlación Spearman: r={r}, p-value={p}\")\n",
        "\n",
        "r, p = stats.kendalltau(df['variablecontinua1'], df['variablecontinua2'])\n",
        "print(f\"Correlación Kendalltau: r={r}, p-value={p}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVHHZXMZxaJ"
      },
      "source": [
        "###**Correlación de todas las variables de data frame**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk57rCbgSHze"
      },
      "source": [
        "Print the columns which are positively correlated with price, \n",
        "from most positive to least positive. \n",
        "Similarly, print the columns which are negatively correlated, \n",
        "from most negative to least negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "dCgJZvnRSaLn",
        "outputId": "494ac34e-56e5-4570-b36c-c5aab7a46adc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-252c6a20fb3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from most positive to least positive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#and from most negative to least negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpos_cor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcorrm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_cor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'corrm' is not defined"
          ]
        }
      ],
      "source": [
        "#Print the columns which are correlated with price, \n",
        "#from most positive to least positive. \n",
        "#and from most negative to least negative\n",
        "pos_cor = corrm['price'] >0\n",
        "corrm['price'][pos_cor].sort_values(ascending = False).to_frame()\n",
        "\n",
        "neg_cor = corrm['price'] <0\n",
        "corrm['price'][neg_cor].sort_values(ascending = True).to_frame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWRml-RKaAvH"
      },
      "source": [
        "###**Correlación para datos agrupados por categorías**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pG71WjGTaYw"
      },
      "outputs": [],
      "source": [
        "#Sacar la correlación entre las variables para una categoría específica de la varcat1\n",
        "varcat1 = df[df[\"varcat1\"] == \"categoria1varcat1\"]\n",
        "\n",
        "central = varcat1[\n",
        "    (varcat1[\"datos\"] >= df.quantile(q=0.25))\n",
        "    & (varcat1[\"datos\"] <= df.quantile(q=0.75))\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlHJOoGIa9ES"
      },
      "source": [
        "o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUlgNe9pa6Jm"
      },
      "outputs": [],
      "source": [
        "#Let's find the correlation between price and parking for each neighborhood\n",
        "cbn = df.groupby(\"neighbourhood\")[[\"price\", \"parking\"]].corr()\n",
        "cbn\n",
        "\n",
        "#Let's filter out redundant information\n",
        "\n",
        "cbn = cbn.reset_index()\n",
        "cbn = cbn.drop(columns=[\"parking\"])\n",
        "cbn.columns = [\"neighbourhood\", \"variable\", \"r_parking_price\"]\n",
        "cbn = cbn[cbn[\"variable\"]==\"parking\"]\n",
        "cbn = cbn.drop(columns=[\"variable\"])\n",
        "cbn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HySVCNwAbkLA"
      },
      "outputs": [],
      "source": [
        "\"\"\"Find out how many neighborhoods present a strongly negative, \n",
        "mildly negative, mildly positive, and strongly positive correlation \n",
        "between price and parking. Specifically, \n",
        "we want to know how many neighborhoods show a correlation between \n",
        "-1 and -0.5, between -0.5 and 0, between 0 and 0.5 and between 0.5 and 1.\"\"\"\n",
        "\n",
        "cbn[\"r_parking_price\"].plot.hist(\n",
        "    bins=[-1, -0.5, 0, 0.5, 1],\n",
        "    title=\"Histogram of neighborhood-level correlations for price and parking\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmZGQAE4bi3T"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArZJNGi3IPkq"
      },
      "source": [
        "# **HTML**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZINNcbIoQUho"
      },
      "source": [
        "BeautifulSoup library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNXpKnxpJZid"
      },
      "source": [
        "**Basics of Hyper Text Markup Language**\n",
        "\n",
        "An HTML document is made up of a series of tags. These tags instruct a browser on how to display content to the user. Different tags will cause different output styles to be displayed.\n",
        "\n",
        "Let's begin by discussing a simple HTML formatted string, custom_html_doc.\n",
        "\n",
        "While there are a wealth of tags available in HTML, the above example highlights the fundamentals we need to get started with the language. The four vital tags of any HTML document inlcude:\n",
        "\n",
        "< html > Instructs the browser that your web page is in HTML format.\n",
        "\n",
        "< head > This is information that can be used by external sources (such as search engines). Holds webpage metadata.\n",
        "\n",
        "< title > Viewers see the title in the browser toolbar, when the page is added to favorites, and in search engine results.\n",
        "\n",
        "< body > Defines the body block, which contains the content of the page, including text and images.\n",
        "Other structurally useful tags include:\n",
        "\n",
        "< p > Defines a paragraph block which primarily contains text to be displayed to the user\n",
        "\n",
        "< a > Defines a hyperlink\n",
        "\n",
        "< h1 > Defines an important header\n",
        "\n",
        "< h2 > Define a less important header\n",
        "\n",
        "< br > Define a line break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49iPO4WhUAZU"
      },
      "source": [
        "###**Ver código de una página**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MrQQB6KM35L"
      },
      "source": [
        "Para ver el código HTLM de una página Clic derecho ver código fuente, o ctrl + U o F12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuVSzSz5JgwF"
      },
      "outputs": [],
      "source": [
        "custom_html_doc = \"\"\"\n",
        "<html>\n",
        "<head>\n",
        "<title>HTML Page Title</title>\n",
        "</head>\n",
        "<h1>Head: Important Header: Global News</h1>\n",
        "<br>\n",
        "<h2>Head: Less Imporant Header: Global News</h2>\n",
        "<body>\n",
        "<p class=\"title\"><b>Paragraph: Financial news</b></p>\n",
        "<p class=\"story\"> Stocks had a volatile week, where\n",
        "<a href=\"https://finance.yahoo.com/quote/duk/\" target=\"_blank\" class=\"stock\" id=\"link1\">DUK</a>,\n",
        "<a href=\"https://finance.yahoo.com/quote/d/\" target=\"_blank\" class=\"stock\" id=\"link2\">D</a>,\n",
        "<a href=\"https://finance.yahoo.com/quote/exc/\" target=\"_blank\" class=\"stock\" id=\"link3\">EXC</a>,\n",
        "<a href=\"https://finance.yahoo.com/quote/nee/\" target=\"_blank\" class=\"etf\" id=\"link4\">NEE</a>,\n",
        "<a href=\"https://finance.yahoo.com/quote/so/\" target=\"_blank\" class=\"stock\" id=\"link5\">SO</a>,\n",
        "were all making headlines.</p>\n",
        "<p class=\"details\">End of HTML document.</p>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUalAdjMJz4t"
      },
      "source": [
        "We can view how custom_html_doc will render using the method HTML():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN7_gXWlJzkP"
      },
      "outputs": [],
      "source": [
        "# View the HTML as it would appear by a web browser\n",
        "HTML(custom_html_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k7P4tqwKiSw"
      },
      "source": [
        "The following HTML document was found with all of its end tags missing. Starting from top to bottom, determine the correct order in which end tags should be added to eliminate the issues with the document.\n",
        "```python\n",
        "<h1>This is a Heading\n",
        "<p>This is a paragraph.\n",
        "<br>\n",
        "<p>Another paragraph\n",
        "<br>\n",
        "\n",
        "```\n",
        "(c) < /h1 >, < /p >, < /p >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjapXRrnJGIw"
      },
      "source": [
        "#**Dash**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqa47kwBJLwI"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import clear_output\n",
        "#Ver la terminal más limpia\n",
        "export PS1='$'\n",
        "#Ver la ubicación donde estoy\n",
        "pwd\n",
        "#ver qué se esta corriendo\n",
        "ps ux\n",
        "#Ver qué estan corriendo todos los uduarios\n",
        "ps aux\n",
        "#Ver qué estan corriendo específicamente que tenga la palabra hello\n",
        "ps ux | grep Hello\n",
        "#Parar lo que se está corriendo códico 000\n",
        "Kill 000\n",
        "#Limpiar la terminal\n",
        "clear_output\n",
        "# ir a algun lado\n",
        "cd lugar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-MWhhRmzfu5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# **Comandos de texto básicos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOkS3Lbzt00X"
      },
      "source": [
        "## Los numerales\n",
        "### indican el tamaño\n",
        "####  del título y \n",
        "#####  son usados por Google Colab\n",
        "######   para crear su tabla de contenido.\n",
        "\n",
        "Se puede usar texto con estilo de código en una línea: `!python --version`\n",
        "\n",
        "O como un bloque y con estilo de un lenguaje como _Python_:\n",
        "```python\n",
        "print('Hello World!')\n",
        "```\n",
        "\n",
        "\n",
        "Incluso en negrilla: **`print('Hello World!)`**\n",
        "\n",
        "> Se puede definir bloques de texto en formato de cita o anotación.\n",
        "\n",
        "Se pueden definir listas sin orden:\n",
        "*  Usando asteriscos\n",
        "-  y líneas.  \n",
        "\n",
        "También se pueden definir listas ordenadas\n",
        "1. Usando los números...\n",
        "2. que correspondan...\n",
        "3. con cada elemento.\n",
        "  1. Incluso se pueden\n",
        "  2. hacer listas anidadas\n",
        "    * Usando tanto listas ordenadas como no ordenadas, y\n",
        "    * ...usando la sangría o _indentación_ (dejar espacios o tabulaciones al inicio de la línea).\n",
        "\n",
        "También se pueden agregar enlaces a [sitios web](https://www.python.org/).\n",
        "\n",
        "Y agregar imágenes a partir de una _URL_:\n",
        "  \n",
        "![Python logo](https://www.python.org/static/community_logos/python-logo-master-v3-TM.png)\n",
        "\n",
        "\n",
        "<div>\n",
        "<img src=\"https://natmeurer.com/content/images/2019/11/big.png\" width=\"500\"/>\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "My python Library",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
